\documentclass{beamer}
%\usetheme[secheader]{Lab2C}
\usetheme{Lab2C} % disable pgf drawing to speed up compilation in development
\usepackage{graphicx}
\usepackage{array}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
\usepackage{algorithmic}
\usepackage{amssymb}

\DeclareMathOperator{\Bern}{Bern}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\lstset{
numbers=left,
numbersep=5pt,
numberstyle=\tiny\color{mygray}
}

\newif\ifbeamer
\beamertrue
%\setbeamertemplate{footline}[frame number]
\logo{\includegraphics[height=0.8cm]{ieee_logo.eps}}
\title[Error Rate of Stochastic Block Model with Symmetric Side Information]{On the Optimal Error Rate of Stochastic Block Model with Symmetric Side Information}
\author{Feng Zhao\inst{1} \and Jin Sima\inst{2}\and Shao-Lun Huang\inst{3}}
\institute{\inst{1}Dept. of Electronic Engineering, Tsinghua University
\and\inst{2}Department of Electrical Engineering, California Institute of Technology
	\and \inst{3}Tsinghua-Berkeley Shenzhen Institute, Tsinghua University 
	\\ \vskip 0.5cm ITW 2021}
\date{\today}
\begin{document}
\begin{frame}
	\titlepage
\end{frame}
%\section*{Outline}
\begin{frame}
	\tableofcontents
\end{frame}
\section{Background}
\begin{frame}{Community Detection}
	\begin{columns}
		\column{0.5\textwidth}
		\begin{figure}
			\includegraphics[width=0.8\textwidth]{cd.png}
		\end{figure}
	\column{0.5\textwidth}
	Community Detection is inferring the group of vertices which are more
	densely connected in a graph
	\end{columns}
	\begin{columns}
		\column{0.33\textwidth}
		\begin{figure}
			\includegraphics[width=\textwidth]{benno2t.pdf}
		\end{figure}
		\column{0.05\textwidth}
		$\Rightarrow$
		\column{0.5\textwidth}
		\begin{figure}
		\includegraphics[width=\textwidth]{bennot.pdf}
		\end{figure}
	\end{columns}
\end{frame}
\begin{frame}{Community Detection with Side Information}
	\begin{columns}
		\column{0.5\textwidth}
	\begin{figure}
		\includegraphics[width=0.8\textwidth]{si.png}
		\caption{\scriptsize Graph Model with Side Information}
	\end{figure}
	\column{0.5\textwidth}
	Incorporating side information in graph model
	can improve the performance of Community Detection
	\begin{block}{Objective}
		Investigate quantitatively how side information influences
		the detection error rate for a specific graph model
	\end{block}
	\end{columns}
\end{frame}
\begin{frame}{Binary Symmetric Stochastic Block Model}
	\begin{block}{Characteristics}
		\begin{itemize}
		\item a probabilistic model to generate random graph
		\item larger probability for the existence of edges within the same community
		\item smaller probability for the existence of edges between different communities
		\end{itemize}
		\end{block}
		\begin{columns}
			\column{0.33\textwidth}
				\begin{figure}
				\includegraphics[width=\textwidth]{sbm.png}
			\end{figure}
		\column{0.67\textwidth}
		\quad$(G,Y)\sim \textrm{SSBM}(n, p, q)$
		\begin{itemize}
			\item $Y$: community labels
			\item $n$: the number of nodes
			\item $p$: probability of connecting within clusters
			\item $q$: probability of connecting across clusters
		\end{itemize}
		\end{columns}	
\end{frame}
\begin{frame}{Detection Algorithms and Recovery Metrics}
	\begin{description}
		\item[Observation] random graph $G$, generated by $\textrm{SSBM}(n,p,q)$;
		\item[Estimator] $\hat{Y}(G)$, an algorithm to recover node labels $Y$;
		\item[Error probability] $P_e=P(\hat{Y} \neq \pm Y)$
	\end{description}
	
	\begin{block}{Known Results [Abbe-Bandeira-Hall, 2014]}
		\begin{itemize}
		\item Community labels $Y$ are symmetric: half $+1$, half $-1$;
		\item $p = a\frac{ \log n}{n}, q = b \frac{ \log n}{n}$;
		$\sqrt{a} - \sqrt{b} > \sqrt{2}$
		\item $P_e \to 0$ as $n \to \infty$.
		\end{itemize}
	\end{block}
	\begin{block}{Problem of Error Rate}
	 Does $\lim_{n \to \infty}\frac{\log P_e}{\log n}$
	 exist ?
	\end{block}
\end{frame}
\begin{frame}{Related Works}
	% must mention the work of concurrent submission and explain
	% the difference between the two
	\begin{enumerate}
		\item Phase transition study on SBM with side information
		[Saad-Nosratinia, 2018]
		\item Efficient SDP algorithm to achieve exact recovery
		[Sima-Feng, concurrent submission]
		\item Weak recovery error rate in SBM
		[Zahng-Zhou, 2016]
	\end{enumerate}
	\begin{block}{This Work}
		\begin{enumerate}
			\item Optimal error rate study
			\item Does not take implementation of algorithm into consideration
			\item Strong recovery scenario with side information
		\end{enumerate}
	\end{block}
\end{frame}
\section{Problem Formulation}
\begin{frame}
\frametitle{Binary Symmetric SBM with Side Information}
\begin{block}{Characteristics of SBMSI}
\begin{itemize}
	\item   Each node includes multiple additional observations with
	finite-cardinality alphabet.
	\begin{equation*}
		x_{i1}, x_{i2}, \dots, x_{im} \in \{1, 2, \dots, |\mathcal{X}|\}
	\end{equation*}
	\item The observations $X_i$ at $i$-th node are sampled from distribution $P_0$
	or $P_1$ depending on the node label $Y_i$.
	\begin{equation*}
		X_{i1}, X_{i2}, \dots, X_{im} \textrm{ i.i.d. } \sim P_0 \textrm{ or } P_1
	\end{equation*}
	\item $X$ are conditionally independent of the random
	graph $G$.
	\begin{equation*}
		P(G, X | Y) = P(G | Y) P(X | Y)
	\end{equation*}
\end{itemize}
\end{block}

\end{frame}
\begin{frame}\frametitle{Exact Recovery Error Rate}
Paired hypothesis testing problem:
\begin{equation*}
\begin{cases}
H_0: X \sim P_0=(P \times Q)\\
H_1: X \sim P_1=(Q \times P)
\end{cases}
\end{equation*}
\begin{itemize}
\item Random variable $X=(X_1, X_2)$: $X_1 \sim P, X_2 \sim Q, X_1 \independent X_2$
\item Chernoff information for optimal test $\widehat{H}$
\begin{align*}
&-\lim_{n\to\infty} \frac{1}{n}\log P_e = -\min_{\lambda \in [0,1]}
\log \sum_{x,y\in \mathcal{X}}
P^{1-\lambda}_0(x,y) P^{\lambda}_1(x,y)  \\
&= -\min_{\lambda \in [0,1]}
\left(\log \sum_{x\in \mathcal{X}}
P^{1-\lambda}(x) Q^{\lambda}(x) 
+\log \sum_{y\in \mathcal{X}}
Q^{1-\lambda}(y) P^{\lambda}(y) 
\right)
\end{align*}
\item $\lambda=\frac{1}{2}$: minimizer
\end{itemize}
\end{frame}
\begin{frame}
Paired hypothesis testing problem:
\begin{equation*}
\begin{cases}
H_0: X \sim P_0=(P \times Q)\\
H_1: X \sim P_1=(Q \times P)
\end{cases}
\end{equation*}
Chernoff information for optimal test $\widehat{H}$
\begin{equation*}
-\lim_{n\to \infty} \frac{1}{n}\log P_e = -2 \log \sum_{x\in \mathcal{X}} \sqrt{P(x)Q(x)}
\end{equation*}
Rényi divergence with order $\frac{1}{2}$
\begin{equation*}
D_{1/2}(P||Q) := -2 \log \sum_{x\in \mathcal{X}} \sqrt{P(x)Q(x)}
\end{equation*}
\end{frame}
\section{Sketch of Proof}
\begin{frame}
\frametitle{
Stochastic Block Model} A probabilistic model to generate random graph
\begin{itemize}
\item $Y_i$: label for the $i$-th node
\item $X_{ij}=1$: an edge exists between node $i$ and $j$
\end{itemize}
Procedures:
\begin{enumerate}
\item Generate $Y_1, \dots, Y_n$ uniformly from $\{\pm 1\}^n$
\item Make sure $\sum_{i=1}^n Y_i = 0$
\item $X_{ij} \sim \Bern(p) $ if $Y_i=Y_j$
\item $X_{ij} \sim \Bern(q) $ if $Y_i \neq Y_j$
\end{enumerate}
Misclassfication of label of one node
\begin{itemize}
\item $Y_3, \dots, Y_n$ are given, satisfying $Y_3 + \dots + Y_n  = 0$
\item What's the error rate of the optimal estimator for $Y_1$ and $Y_2$?
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{
Hypothesis Testing in Stochastic Block Model} 
Paired hypothesis testing problem:
\begin{equation*}
\begin{cases}
H_0: Y_1 = 1 \textrm{ and } Y_2 = -1 \iff X \sim \Bern(p) \times \Bern(q)\\
H_1: Y_1 = -1 \textrm{ and } Y_2 = 1 \iff X \sim \Bern(q) \times \Bern(p)
\end{cases}
\end{equation*}
\begin{itemize}
\item $n-1$ i.i.d.  observations of $X$
\item Chernoff information for optimal test $\widehat{H}$
\begin{equation*}
-\lim_{n\to\infty} \frac{1}{n}\log P_e = -2 \log (\sqrt{pq}+\sqrt{(1-p)(1-q)})
\end{equation*}
\item What if $p,q$ varies with $n$?
\end{itemize}
\end{frame}
\begin{frame}
\begin{theorem}[Cramér Theorem]
$X_1, \dots, X_n  \textrm{ i.i.d.} \sim P$, $\gamma > \mathbb{E}[X_1]$, 
\begin{equation*}
-\lim_{n\to \infty}\frac{1}{n} \log P\left(\frac{X_1+ \dots + X_n}{n} > \gamma \right)
= \psi_P^*(\gamma)
\end{equation*}
\end{theorem}
Chernoff Information
\begin{itemize}
\item $X, X_1, \dots, X_n \textrm{ i.i.d.} \sim P_0$
\item $\ell(X) = \log\frac{P_1(X)}{P_0(X)} \sim P$
\end{itemize}
\begin{equation*}
-\lim_{n\to\infty} \frac{1}{n}\log P_e = \psi^*_P(0)
\end{equation*}
\end{frame}
\begin{frame}
\begin{theorem}[Gärtner Ellis Theorem]
$X, X_1, \dots, X_n  \textrm{ i.i.d.} \sim P_n$, $\gamma > \lim_{n\to\infty} \frac{n}{\gamma_n}\mathbb{E}[X_1]$, 
\begin{equation*}
-\lim_{n\to \infty}\frac{1}{\textcolor{red}{\gamma_n}} \log P\left(\frac{X_1+ \dots + X_n}{\textcolor{red}{\gamma_n}} > \gamma \right)
= \psi_P^*(\gamma)
\end{equation*}
\begin{itemize}
\item $\lim_{n\to\infty} \gamma_n = +\infty$
\item Distribution $P_n$ depends on $n$
\item log-MGF: $\psi_P(\lambda)=\lim_{n\to\infty} \frac{n}{\gamma_n} \log \mathbb{E}[e^{\lambda X}]$
\end{itemize}
\end{theorem}
Chernoff Information
\begin{itemize}
\item $X, X_1, \dots, X_n \textrm{ i.i.d.} \sim P_{0,n}$
\item $\ell(X) = \log\frac{P_{1,n}(X)}{P_{0,n}(X)} \sim P_n$
\end{itemize}
\begin{equation*}
-\lim_{n\to\infty} \frac{1}{\textcolor{red}{\gamma_n}}\log P_e = \psi^*_P(0)
%=-\min_{\lambda\in[0,1]}\log [\textcolor{red}{\lim_{n\to\infty}\frac{n}{\gamma_n}} \sum_{x\in \mathcal{X}} P_{0,n}^{1-\lambda}(x)P_{1,n}^{\lambda}(x)]
\end{equation*}
\end{frame}
% \begin{frame}\frametitle{Hypothesis Testing}
% Paired hypothesis testing problem:
% \begin{equation*}
% \begin{cases}
% H_0: X \sim P_{0,n}=(P_n \times Q_n)\\
% H_1: X \sim P_{1,n}=(Q_n \times P_n)
% \end{cases}
% \end{equation*}
% Random variable $X=(X_1, X_2)$: $X_1 \sim P_n, X_2 \sim Q_n, X_1 \independent X_2$

% Chernoff information for optimal test $\widehat{H}$
% \begin{align*}
% &-\lim_{n\to\infty} \frac{1}{\gamma_n}\log P_e = -\min_{\lambda \in [0,1]}
% \log[\lim_{n\to\infty}\frac{n}{\gamma_n} \sum_{x,y\in \mathcal{X}}
% P^{1-\lambda}_{0,n}(x,y) P^{\lambda}_{1,n}(x,y) ] \\
% &=  -\min_{\lambda \in [0,1]}
% \log[\lim_{n\to\infty}\frac{n}{\gamma_n} \left((\sum_{x\in \mathcal{X}}
% P^{1-\lambda}_{n}(x) Q^{\lambda}_{n}(x)) \cdot
% (\sum_{y\in \mathcal{X}}
% Q^{1-\lambda}_{n}(y) P^{\lambda}_{n}(y)\right) ]
% \end{align*}
% \end{frame}
\begin{frame}
\frametitle{
Stochastic Block Model}
\begin{itemize}
\item $Y_i$: label for the $i$-th node
\item $X_{ij}=1$: an edge exists between node $i$ and $j$
\item $\color{red}  p_n=\frac{a \log n}{n}, q_n =\frac{b \log n}{n} $
\end{itemize}
Procedures:
\begin{enumerate}
\item Generate $Y_1, \dots, Y_n$ uniformly from $\{\pm 1\}^n$
\item Make sure $\sum_{i=1}^n Y_i = 0$
\item $X_{ij} \sim \Bern(\textcolor{red} {p_n} ) $ if $Y_i=Y_j$
\item $X_{ij} \sim \Bern(\textcolor{red}{q_n} ) $ if $Y_i \neq Y_j$
\end{enumerate}
Misclassfication of label of one node
\begin{itemize}
\item $Y_3, \dots, Y_n$ are given, satisfying $Y_3 + \dots + Y_n  = 0$
\item What's the error rate of the optimal estimator for $Y_1$ and $Y_2$?
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{
Hypothesis Testing in Stochastic Block Model} 
Paired hypothesis testing problem:
\begin{equation*}
\begin{cases}
H_0: Y_1 = 1 \textrm{ and } Y_2 = -1 \iff X \sim P_{0,n} = \Bern(p_n) \times \Bern(q_n)\\
H_1: Y_1 = -1 \textrm{ and } Y_2 = 1 \iff X \sim P_{1,n} = \Bern(q_n) \times \Bern(p_n)
\end{cases}
\end{equation*}
Choose $\gamma_n = \log n$
\begin{align*}
\psi_P(\lambda) &= \lim_{n\to \infty} \frac{n}{\log n} \log E_{P_{0,n}} [e^{\lambda \ell(X)}]\\
&=a^{1-\lambda}b^{\lambda}
+a^{\lambda}b^{1-\lambda} -a-b
\end{align*}
Polynomial error rate
\begin{equation*}
-\lim_{n\to\infty} \frac{1}{\log n}\log P_e = -\min_{\lambda} \psi_P(\lambda) = (\sqrt{a}-\sqrt{b})^2
\end{equation*}
\end{frame}
\section{Conclusion}
\begin{frame}
\frametitle{Conclusion}
\begin{itemize}
\item In paired hypothesis testing, Chernoff information $\Rightarrow$ Rényi divergence with order $\frac{1}{2}$
\item Gärtner Ellis Theorem generalizes Cramér Theorem, allowing the derivation of polynomial error rate
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{}
\begin{block}{}
\centering
{\Huge Questions and Answers}
\end{block}
\end{frame}
\end{document}
